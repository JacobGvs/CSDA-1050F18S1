---
title: "Socioeconomics of Pedestrian Accidents"
output: html_notebook
---


Start by loading the various libraries and packages we will be using.

The "Pacman" package will automatically install any packages you do not currently have installed and load the applicable libraries.

```{r message=FALSE, warning=FALSE, error=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, spdplyr, sf, sp, spdep, spatialEco, rgdal, geojsonio, geojsonR, leaflet, leaflet.minicharts, leaflet.extras,leafsync, maptools, tmap, tidyverse, ggplot, gclus, rpart)
```


## Create our Data Tables

#### KSI Pedestrian Dataset

This dataset is a subset of the Killed and Seriously Injured (KSI) dataset collected by the Toronto Police Service from 2008-2018. These events include any serious or fatal collisions where a Pedestrian is involved. To learn more about Pedestrians related collisions in Toronto you can follow this link: http://data.torontopolice.on.ca/pages/pedestrians

We will use the FROM_GeoJson command from the "geojsonR" package to download a json file from the provided URL. The geojson_read command from the "geojsonio" and formating form the "sp" package can then be used to read the json and generate a SpatialPointsDataFrame 
```{r}
ksi_ped <- geojsonio::geojson_read("https://opendata.arcgis.com/datasets/3dedc9bff625450990b8d480f397ad3f_0.geojson", what = "sp")
head(ksi_ped)
```

#### KSI TTC/Municipal Vehicle Dataset

This dataset is a subset of the Killed and Seriously Injured (KSI) dataset collected by the Toronto Police Service from 2008-2018. These events include any serious or fatal collision involving an operator or passenger of a TTC, Transit Vehicle, streetcar or Municipal Vehicle. To learn more about TTC-Municipal Vehicle related collisions in Toronto you can follow this link: http://data.torontopolice.on.ca/pages/ttc-municipal-vehicle

We will use the FROM_GeoJson command from the "geojsonR" package to download a json file from the provided URL. The geojson_read command from the "geojsonio" and formating form the "sp" package can then be used to read the json and generate a SpatialPointsDataFrame 
```{r}
ksi_ttc <- geojsonio::geojson_read("https://opendata.arcgis.com/datasets/dc4751278e604d65b0886b9765d4b551_0.geojson", what = "sp")
head(ksi_ttc)
```

#### Merge KSI_TTC and KSI_Ped

As you will have noticed from the descritpions both the KSI_PED and KSI_TTC datasets are themselves subsets of the Toronto Police Services' KSI Dataset. We downloaded them as seperate dataframes to enable faster downloads as they make up a small portion of the full KSI dataset, which would take significantly more time to download and sort.
Because they come from the same base dataset and have the same schema we can merge them back into one dataframe to work with. We will merge them based on the "Index_" to ensure there are no duplicates created due to the merging process.
```{r}
ksi_merged <- merge(ksi_ped,ksi_ttc, by="Index_")
```


#### Neighbourhood Boundaries - City of Toronto

The City of Toronto also maintains a list that defines the boundaries of all the neighbourhoods in the city. A file containing the spatial data required to map these neighbourhoods can be downloaded form the City of Toronto open data portal. Due to how this file will download, unlike the KSI data, we cannot directly read the GeoJson file but have to download it before the file can be read.
```{r echo=FALSE, message=FALSE, warning=FALSE}
set_wd <- function() {
  library(rstudioapi) # make sure you have it installed
current_path <- getActiveDocumentContext()$path 
setwd(dirname(current_path))
print( getwd() )
}

download.file("https://ckan0.cf.opendata.inter.prod-toronto.ca/download_resource/a083c865-6d60-4d1d-b6c6-b0c8a85f9c15?format=geojson&projection=4326", destfile = "datasets/Neighbourhoods.geojson", )

nbh <- geojsonio::geojson_read("datasets/Neighbourhoods.geojson", what = "sp")

head(nbh)
```

#### Simply Analytics - Census Data

From Simply Analytics i have downloaded shapefiles containing average income by  census track and subdivision. These two shapefiles will usefull in drilling down beyond the neighbourhood.
Both shapefiles can be uploaded into a spatial dataframe using the st_read command from the "sf" package we installed earlier then converting them to a SpatialPolygonsDataFrame.

```{r}
ogrInfo(dsn="datasets/SimplyAnalytics_C1", layer="C1")
ctr <- readOGR(dsn = "datasets/SimplyAnalytics_C1", layer = "C1")
names(ctr@data)[names(ctr@data)=="VALUE0"] <- "Household Total Income After-Tax"
names(ctr@data)[names(ctr@data)=="VALUE1"] <- "Household Aggregate Income"
names(ctr@data)[names(ctr@data)=="VALUE2"] <- "Household Average Income"
head(ctr)
```

```{r}
ogrInfo(dsn="datasets/SimplyAnalytics_C2", layer="C2")
disem <- readOGR(dsn = "datasets/SimplyAnalytics_C2", layer = "C2")
names(disem@data)[names(disem@data)=="VALUE0"] <- "Household Total Income After-Tax"
names(disem@data)[names(disem@data)=="VALUE1"] <- "Household Aggregate Income"
names(disem@data)[names(disem@data)=="VALUE2"] <- "Household Average Income"
summary(disem)
head(disem@data)
```

#### Additional Datatables

One additional datatable may be needed depending on how we choose to cluster the KSI data.
At a high level grouping accidents to a given neighbourhood as defined by the city then using the census data to further cluster by census track and subdivision will be ideal. I may need to pull the Neighbourhood Profiles data set from the city of Toronto to get an average income level by neighbourhood as this does not appear to be avaialble directly from the census datasets.


## Reviewing the Datasets

With our datasets downloaded we can start by looking through the data. Becasue we are dealing almost exclusively with spatial data the easiest way to get a handle on the data is by mapping it.
To do this I will be using features from the "leaflet" package.

#### KSI_merged Dataset

Lets satrt by getting an idea of where accidents are occuring. The leaflet package will map the data for us. To make this more readable I have had the system autocluster the accidents. These clusters don't have any relation to specific neighbourhoods and will need to be adjusted later so that the clusters are in line with our other datasets.
```{r}
leaflet(ksi_merged) %>%
  addTiles() %>%
  addMarkers(lng = ksi_merged$LONGITUDE, lat = ksi_merged$LATITUDE, clusterOptions = markerClusterOptions())
```

This map will allow you to zoom in and the clusters will auto adjust as you zoom in and out. These clusters are based on proximity to a central point. Once you get to the lowest zoom levels, clicking on a cluster will map the individual accidents. Details for each accident are not currenlty included in the mapping.


#### Neighbourhood Boundaries - City of Toronto

Our next dataset contains the boundary lines for various neighbourhoods in Toronto. mapping this will begin to give some dimension to how we intend to cluster accidents going forward.
```{r}
leaflet(nbh) %>%
  addTiles() %>%
  addPolygons()
```

#### Simply Analytics - Census Data

We can similarly plot both the census tract and dissemination area files pulled from Simply Analytics.
```{r}
leaflet(ctr) %>%
  addTiles() %>%
  addPolygons()
```

```{r}
leaflet(disem) %>%
  addTiles() %>%
  addPolygons()
```

Our next step will be to assing each accident in the KSI data to a dissemination area polygon. But before we can do that we will first have to clean up the various datasets.


## Cleaning the Data

Before we begin we need to ensure that our various datasets are compatible.

Using the CRS fuction in the SF package We can check the coordinate reference system being used by our main files and can see that they are all the same.
```{r}
st_crs(ksi_merged)
st_crs(ctr)
st_crs(disem)
st_crs(nbh)
```

We luckily all the files we have downloaded use the same co-ordinate reference system so we will be able to compare and associate them to one another without having to re-project them into a comon co-ordinate system. This is especially convenient given that the mapping package w are using "leaflet" does not support "CRS" one of the other reference systems.

### Cleaning the KSI data

As we review the datasets one thing you may have noticed is that the KSI dataset contains a large number of potenitally duplicate entries.

```{r}
head(ksi_merged@data, 20)
```

While each accident is assigned a different index id number(Index_), in most cases multiple index numbers are assigned to the same account number(ACCNUM). This is because, for each accident, the driver of the vehicle, the pedestrian, and any other applicable individual related to the loss has their information recorded to the dataset. This is most evident if you look at the involved vehicle type (INVTYPE) columns.

Since this review is focused on pedestrian accidents, we will filter our KSI_Merged file to include only instances where the column INVTYPE is a pedestrian.

We can easily identify all unique items in the INVTYPE column and select those that are representative of pedestrians.

```{r}
unique(ksi_merged@data$INVTYPE)
```

If we look at our list of options, there are a few different entries we should be considering to be a pedestrian.
For this review we will be filtering out all entries where the INVTYPE is "Pedestrian", "Pedestrian - Not Hit", "In-Line Skater", or "Wheelchair".

So as not to lose our merged dataset, we will also create a new filtered datatable.
We can easily identify and, if required in the future, modify the list of items being filtered into our datatable by setting them as a target
```{r}
target_invtype <- c("Pedestrian", "Pedestrian - Not Hit", "In-Line Skater", "Wheelchair")
ksi_modified <- ksi_merged[ksi_merged@data$INVTYPE %in% target_invtype,]
head(ksi_modified@data, 20)
```

There are also a number of columns that will not be needed for our review and can be removed from our modified dataset.
These include: OFFSET, VEHTYPE, MANOEUVER, DRIVACT, DRIVCOND, CYCLISTYPE, CYCACT, CYCCOND, CYCLIST, TRSN_CITY_VEH, TRSN_CITY_, coords.x1, coords.x2

```{r}
target_coll <- c("OFFSET", "VEHTYPE", "MANOEUVER", "DRIVACT", "DRIVCOND", "CYCLISTYPE", "CYCACT", "CYCCOND", "CYCLIST", "TRSN_CITY_VEH", "TRSN_CITY_", "coords.x1", "coords.x2")
ksi_modified <- ksi_modified[,-which(names(ksi_modified@data) %in% c(target_coll))]
head(ksi_modified@data, 20)
```

### Cleaning the Toronto Neighbourhood data

Next we will look at the Toronto Neighbourhood dataset.

```{r}
head(nbh@data, 20)
```

We can see that for the most part all columns in this data set are useful. The columns that will not be of use and can be removed are:
  PARENT_AREA_ID - this field containe no unique values
  AREA_LONG_CODE - this is variable is identical to AREA_SHORT_CODE
  AREA_DESC - this is variable is identical to AREA_NAME
  X - this field containe no unique values
  Y - this field containe no unique values

```{r}
unique(nbh@data$PARENT_AREA_ID)
unique(nbh@data$X)
unique(nbh@data$Y)
```

```{r}
targetnbh <- c("PARENT_AREA_ID", "AREA_LONG_CODE", "AREA_DESC", "X", "Y")
nbh_modified <- nbh
nbh_modified <- nbh_modified[,-which(names(nbh_modified@data) %in% c(targetnbh))]
head(nbh_modified@data, 20)
```

### Cleaning the Census Tract and Disemination data

Next we will look at the Toronto Neighbourhood dataset.

```{r}
head(ctr@data, 20)
```

We can see that these have a very minimal number of fields and will not require any adjustmeents prior to proceeding with our review.


## Associating the Datasets

We are interested in associating all our spatial polygon dataframes to the KSI data points that represent each pedestrian related accident.

Our first task will be to join our KSI dataset to a Toronto Neighbourhood.

This can be done using the spatialEco package. The spatialEco::point.in.poly function intersects point and polygon feature classes and adds polygon attributes to points. This function will re-name columns with similar names so we will also address re-naming some of the newly added columns.

```{r}
ksi_coord <- spatialEco::point.in.poly(ksi_modified, nbh_modified)
names(ksi_coord@data)[names(ksi_coord@data)=="LONGITUDE.x"] <- "LONGITUDE"
names(ksi_coord@data)[names(ksi_coord@data)=="LATITUDE.x"] <- "LATITUDE"
names(ksi_coord@data)[names(ksi_coord@data)=="LONGITUDE.y"] <- "LONGITUDE.nbh"
names(ksi_coord@data)[names(ksi_coord@data)=="LATITUDE.y"] <- "LATITUDE.nbh"
head(ksi_coord@data, 20)
```

Next we will add in the disemination area information. Being the most granular this will allow us to accurately associate an average income to each accident.

```{r warning=FALSE}
ksi_coord <- spatialEco::point.in.poly(ksi_coord, disem)
head(ksi_coord@data, 20)
```

Now that we have a assigned an average income to each accident we can start working to review our data and learn more abot what story it can tell.

Since we are trying to look at socioeconomiv impact on accidents we will first start by focusing on average income.

Lets start by creating a histogram of the Household Average Income information for our accidents.

```{r}
qplot(ksi_coord$Household.Average.Income,
      geom="histogram",
      binwidth = 5,
      main = "Histogram Average Income",
      xlab = "Average Income",
      fill=I("blue"), 
      col=I("red"))
```

As we can see the majority of losses are clustered at the bottom end of the income scale. However this histogram can be deceiving. The average income varies so greatly between the lowest and highest values that it is hard to identify what income brakets this cluster actually represents.

We can re-present the same graph with a reduced bin width but it does little to make the data points more meaningful.

```{r}
qplot(ksi_coord$Household.Average.Income,
      geom="histogram",
      main = "Histogram Average Income",
      xlab = "Average Income",
      fill=I("blue"), 
      col=I("red"))
```


To allow us to more easily review the data set we will also convert it from a SpatialPointsDataFrame to a regular dataframe. We can then pull some basic information about household income.

```{r}
ksi_sf <- as.data.frame(ksi_coord@data)
mean(ksi_sf$Household.Average.Income, na.rm = TRUE)
mode(ksi_sf$Household.Average.Income)
range(ksi_sf$Household.Average.Income, na.rm = TRUE)
quantile(ksi_sf$Household.Average.Income, na.rm = TRUE)
quantile(ksi_sf$Household.Average.Income, na.rm = TRUE, probs = c(0.01, 0.025, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.00))
```

The quantile results are very interesting in that we can see that the bottom 25% of incomes actually varies between $0 and $65,000 and the highest value is well beyond being plausibly compared to the rest

This does give some indication that we will probably want to focus on the bottom 50% of incomes depedning on how the income level corelates to frequency of accidents


```{r warning=FALSE}
ksi_sf <- as.data.frame(ksi_coord@data)

freq <- table(ksi_sf$YEAR, ksi_sf$AREA_NAME, ksi_sf$ROAD_CLASS, ksi_sf$Household.Average.Income)
yearfreq <- ksi_sf %>%
  group_by(YEAR) %>%
  summarise(freq = n())
yearfreq <- as.data.frame(yearfreq)
AreaLossFreq <- ksi_sf %>%
  group_by(AREA_NAME) %>%
  summarise(freq = n())
AreaLossFreq <- as.data.frame(AreaLossFreq)
RoadClassFreq <- ksi_sf %>%
  group_by(ROAD_CLASS) %>%
  summarise(freq = n())
RoadClassFreq <- as.data.frame(RoadClassFreq)
AveIncomeFreq <- ksi_sf %>%
  group_by(Household.Average.Income) %>%
  summarise(freq = n())
AveIncomeFreq <- as.data.frame(AveIncomeFreq)
```

A standard frequency plot will do much better at indicating the average income of the areas where these pedestrian accidents usually occur but again will not provide any relevant data
```{r}
plot(AveIncomeFreq)
```

We will want to use the quantile results to set income bandwiths to apply to each of our accidents. As evident from the hitograms and frequency plot we will also want to look at focusing on the lower quantiles.

To do:

1) Establish income bandwiths and associate back to main datatable.

2) Continue with frequency revie and comparison of various variables.

3) Correct clustering as part of the evaluation. Example below does not help given it cannot be read.

```{r warning=FALSE}
library(pvclust)
d <- dist(ksi_sf, method = "euclidean")
fit <- hclust(d, method="ward.D2")
plot(fit)
groups <- cutree(fit, k=5)
rect.hclust(fit, k=5, border="red")
```

4) Review how to bind markers in leaflet to their polygon(s). Currently unclear if this feature is available in Rstudio or only available using the leaflet code html or leaflet in python

```{r}
Lcontent <- paste("Year:", ksi_coord@data$YEAR,
                  "Income:",ksi_coord@data$Household.Average.Income, 
                  "Location:",
                  ksi_coord@data$STREET1)

leaflet(ksi_coord) %>%
  addTiles() %>%
  addCircleMarkers(
    stroke = FALSE, fillOpacity = 0.5,weight = 1,
    label = ~as.character(Lcontent),
    clusterOptions = markerClusterOptions()) %>%
  addPolygons(data = nbh_modified,
              fillColor = "transparent", 
              color = "#000000",
              fillOpacity = 0.8,
              group = "Neighbourhood", 
              weight = 2) %>%
  addPolygons(data = ctr,
              fillColor = "transparent", 
              color = "#000000",
              fillOpacity = 0.8,
              group = "Census Tract", 
              weight = .8) %>%
  addPolygons(data = disem,
              fillColor = "transparent", 
              color = "#000000",
              fillOpacity = 0.8,
              group = "Disemination Area", 
              weight = .4) %>%
  addLayersControl(overlayGroups =c("Neighbourhood", "Census Tract", "Disemination Area"),
                   options = layersControlOptions(collapsed=FALSE))

```




























